# -*- coding: utf-8 -*-
"""House prices prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VKNjudwuCAUpDYijVjIMgdfhFtmv5grw

# Part 1: Data Preprocessing

Dataset: https://www.kaggle.com/competitions/house-prices-advanced-regression-techniques/data

Importing numpy to deal with arrays , importing pandas to deal with dataset and dataframe , importing matplotlib and seaborn are the data visulization libraries
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

dataset = pd.read_csv('/content/train.csv')

"""# Data Exploration

To get first few lines of dataset
"""

dataset.head()

"""Only sales price is the dependent variable rest other are independent variables so, sales price is the target variable"""

dataset.shape

"""There are total 1460 observation in the dataset and 81 columns

To get the name of all columns in the datset
"""

dataset.columns

dataset.info()

"""we can see the datatypes above as there are in total 3 datsets int64 , object , float64"""

# statistical summary
dataset.describe()

# To get the number of numerical and number of characterical column in dataset
# all numerical columns
dataset.select_dtypes(include=['int64','float64']).columns

len(dataset.select_dtypes(include=['int64','float64']).columns)

# Categorical columns
dataset.select_dtypes(include=['object']).columns

len(dataset.select_dtypes(include=['object']).columns)

"""# Dealing with null values"""

dataset.isnull().values.any()

dataset.isnull().values.sum()
# Total number of null value in this dataset

# number of null value in each column
dataset.isnull().sum()

# columns with null value
dataset.columns[dataset.isnull().any()]

len(dataset.columns[dataset.isnull().any()])

# null values with heatmap

plt.figure(figsize=(16,9))
sns.heatmap(dataset.isnull())
plt.show()

"""The broken white lines can show the null values"""

# To get the null percent of each column
# (missing values/ total values) * 100
null_percent = dataset.isnull().sum()/dataset.shape[0] * 100

null_percent

# Have to drop the column with more than 50 percent null values

cols_to_drop = null_percent[null_percent > 50].keys()

cols_to_drop

dataset = dataset.drop(columns=['Alley', 'MasVnrType', 'PoolQC', 'Fence', 'MiscFeature'])

dataset.shape

len(dataset.columns[dataset.isnull().any()])

# columns with null value
dataset.columns[dataset.isnull().any()]

"""Add columns mean to numerical columns"""

# Numerical columns
# 'LotFrontage', 'MasVnrArea','GarageYrBlt'

dataset['LotFrontage'] = dataset['LotFrontage'].fillna(dataset['LotFrontage'].mean())
dataset['MasVnrArea'] = dataset['MasVnrArea'].fillna(dataset['MasVnrArea'].mean())
dataset['GarageYrBlt'] = dataset['GarageYrBlt'].fillna(dataset['GarageYrBlt'].mean())

len(dataset.columns[dataset.isnull().any()])

"""Add columns mode to categorical columns"""

dataset.select_dtypes(include='object').columns

dataset.columns[dataset.isnull().any()]

len(dataset.columns[dataset.isnull().any()])

# we have to replace the categorical column with column mode

# mode is the most repeated value

dataset['BsmtQual'] = dataset['BsmtQual'].fillna(dataset['BsmtQual'].mode()[0])
dataset['BsmtCond'] = dataset['BsmtCond'].fillna(dataset['BsmtCond'].mode()[0])
dataset['BsmtExposure'] = dataset['BsmtExposure'].fillna(dataset['BsmtExposure'].mode()[0])
dataset['BsmtFinType1'] = dataset['BsmtFinType1'].fillna(dataset['BsmtFinType1'].mode()[0])
dataset['BsmtFinType2'] = dataset['BsmtFinType2'].fillna(dataset['BsmtFinType2'].mode()[0])
dataset['Electrical'] = dataset['Electrical'].fillna(dataset['Electrical'].mode()[0])
dataset['FireplaceQu'] = dataset['FireplaceQu'].fillna(dataset['FireplaceQu'].mode()[0])
dataset['GarageType'] = dataset['GarageType'].fillna(dataset['GarageType'].mode()[0])
dataset['GarageFinish'] = dataset['GarageFinish'].fillna(dataset['GarageFinish'].mode()[0])
dataset['GarageQual'] = dataset['GarageQual'].fillna(dataset['GarageQual'].mode()[0])
dataset['GarageCond'] = dataset['GarageCond'].fillna(dataset['GarageCond'].mode()[0])

len(dataset.columns[dataset.isnull().any()])

dataset.isnull().values.any()

"""# Distplot"""

# distplot of the target variables

plt.figure(figsize=(16,9))
bar = sns.distplot(dataset['SalePrice'])
bar.legend(["Skewness: {:.2f}".format(dataset['SalePrice'].skew())])
plt.show()

"""With skewness we can get the probability distribution above plot shows that most of the house lies in 2 lakh range

# Correlation matrix
"""

dataset_2 = dataset.drop(columns='SalePrice')

dataset_2.shape

numeric_columns = dataset_2.select_dtypes(include=[np.number])  # Select only numeric columns
numeric_columns.corrwith(dataset['SalePrice']).plot.bar(
    figsize=(16,9), title='Correlated with SalePrice', grid=True
)

# Select only the numeric columns
numeric_data = dataset.select_dtypes(include=[np.number])

# Generate the heatmap
plt.figure(figsize=(25,25))
ax = sns.heatmap(data=numeric_data.corr(), cmap='coolwarm', annot=True, linewidths=2)
plt.show()

"""Darker the color of the cell means these 2 cells are more correlated"""

# Select only numeric columns
numeric_data = dataset.select_dtypes(include=[np.number])

# Calculate the correlation matrix
high_corr = numeric_data.corr()

high_corr_features = high_corr.index[abs(high_corr['SalePrice'])>0.5]

high_corr_features

len(high_corr_features)

numeric_data = dataset.select_dtypes(include=[np.number])
plt.figure(figsize=(16,9))
ax = sns.heatmap(data=numeric_data[high_corr_features].corr(), cmap='coolwarm', annot=True, linewidths=2)
plt.show()

"""The box with same color and nearly same number are highly correlated

# Dealing with the categorical values
"""

dataset.shape

# categorical columns
dataset.select_dtypes(include='object').columns

len(dataset.select_dtypes(include='object').columns)

# Encoding all the categorical columns
dataset = pd.get_dummies(data=dataset, drop_first= True)

dataset.shape

# categorical columns
dataset.select_dtypes(include='object').columns

len(dataset.select_dtypes(include='object').columns)

"""# Splitting the dataset into Train and Test set"""

# Getting the independent variable or matrix of features
x = dataset.drop(columns='SalePrice')

# target varibale or dependent variable
y = dataset['SalePrice']

from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(x,y, test_size = 0.2, random_state=0)
# test_size=0.2: This specifies that 20% of the data should be used for testing, while the remaining 80% will be used for training
# random_state=0: This ensures that the split is reproducible. Every time you run the code with the same random_state, youâ€™ll get the same split.

x_train.shape

y_train.shape

x_test.shape

y_test.shape

"""# Feature Scaling"""

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
x_train = sc.fit_transform(x_train)
x_test = sc.transform(x_test)

x_train

x_test

"""# Part2: Building the model

# 1- Multiple linear regression
"""

from sklearn.linear_model import LinearRegression
regressor_mlr = LinearRegression()
regressor_mlr.fit(x_train,y_train)

y_pred = regressor_mlr.predict(x_test)

from sklearn.metrics import r2_score

r2_score(y_test,y_pred)

"""This shows that our model is not working great

# 2- Random Forest Regression
"""

from sklearn.ensemble import RandomForestRegressor
regressor_rf = RandomForestRegressor()
regressor_rf.fit(x_train, y_train)

y_pred = regressor_rf.predict(x_test)

from sklearn.metrics import r2_score
r2_score(y_test,y_pred)

"""This is much much better than Linear Regression

# 3- XGBoost regression
"""

from xgboost import XGBRFRegressor
regressor_xgb = XGBRFRegressor()
regressor_xgb.fit(x_train,y_train)

y_pred = regressor_xgb.predict(x_test)

from sklearn.metrics import r2_score
r2_score(y_test, y_pred)

"""The model whose score is near about 1 is the best model and from above we can conclude that random forest is working best for our model

# Part3: Hyper parameter tuning
"""

from sklearn.model_selection import RandomizedSearchCV
from sklearn.ensemble import RandomForestRegressor

# Example parameter grid for RandomizedSearchCV
param_grid = {
    'n_estimators': [100, 200, 500, 1200],
    'max_depth': [None, 10, 20, 30, 40, 50],
    'max_features': ['auto', 'sqrt'],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'bootstrap': [True, False]
}

# Initialize the RandomForestRegressor
rf = RandomForestRegressor()

# Initialize RandomizedSearchCV
random_cv = RandomizedSearchCV(estimator=rf, param_distributions=param_grid,
                               n_iter=50, cv=5, verbose=2, random_state=42, n_jobs=-1)

# Fit the model
random_cv.fit(x_train, y_train)

random_cv.best_estimator_

random_cv.best_params_

"""# Final Model- Random forest regressor"""

from sklearn.ensemble import RandomForestRegressor
regressor = RandomForestRegressor(max_features='sqrt', n_estimators=1200)
regressor.fit(x_train, y_train)

y_pred = regressor.predict(x_test)

from sklearn.metrics import r2_score
r2_score(y_test, y_pred)

